<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>AutoMathCritique
  </title>
  <link rel="icon" type="image/x-icon" href="static/images/image.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero is-light">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Enhancing LLM Reasoning via Critique Models with
              Test-Time and Training-Time Supervision</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://woooodyy.github.io/" target="_blank" rel="noopener noreferrer">Zhiheng Xi</a>,
              </span>
              <span class="author-block">
                Dingwen Yang,
              </span>
              <span class="author-block">
                Jixuan Huang,
              </span>
              <span class="author-block">
                Jiafu Tang,
              </span>
              <span class="author-block">
                Guanyu Li,
              </span>
              <span class="author-block">
                Yiwen Ding,
              </span>
              <span class="author-block">
                Wei He,
              </span>
              <span class="author-block">
                Boyang Hong,
              </span>
              <span class="author-block">
                Shihan Dou,
              </span>
              <span class="author-block">
                Wenyu Zhan,
              </span>
              <span class="author-block">
                  Xiao Wang,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=7Z0V_SoAAAAJ&hl=en" target="_blank"
                  rel="noopener noreferrer">Rui Zheng</a>,
              </span>
              <span class="author-block">
                  Tao Ji,
              </span>
              <br/>
              <span class="author-block">
                  Xiaowei Shi,
              </span>
              <span class="author-block">
                  Yitao Zhai,
              </span>
              <span class="author-block">
                  Rongxiang Weng,
              </span>
              <span class="author-block">
                  Jingang Wang,
              </span>
              <span class="author-block">
                  Xunliang Cai,
              </span>
              </br>
              <span class="author-block">
                <a href="https://guitaowufeng.github.io/" target="_blank" rel="noopener noreferrer">Tao Gui</a>,
              </span>
              <span class="author-block">
                <a href="https://zxwu.azurewebsites.net/" target="_blank" rel="noopener noreferrer">Zuxuan Wu</a>,
              </span>
              <span class="author-block">
                <a href="http://qizhang.info/" target="_blank" rel="noopener noreferrer">Qi Zhang</a>,
              </span>
              <span class="author-block">
                <a href="https://xpqiu.github.io/" target="_blank" rel="noopener noreferrer">Xipeng Qiu</a>,
              </span>
              <span class="author-block">
                <a href="https://xuanjing-huang.github.io/" target="_blank" rel="noopener noreferrer">Xuanjing
                  Huang</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=f3_FP8AAAAAJ&hl=en" target="_blank"
                  rel="noopener noreferrer">Yu-Gang Jiang</a>,
              </span>
            </div>


            <div class="is-size-5 publication-authors">
              <span class="author-block" style="font-size: 24px;">Fudan University & Meituan</span>
              <!-- <span class="eql-cntrb"><small><br /><sup>*</sup>Indicates Equal Contribution</small></span> -->
              </br>
              <span class="author-block">Correspondence to: zhxi22@m.fudan.edu.cn , {tgui,qz}@fudan.edu.cn</span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2411.16579.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>


                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/hotdog-zz/Mathcritique" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2411.16579" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/MathCritique/MathCritique-76k" target="_blank"
                    class="external-link button is-normal is-rounded is-dark" rel="noopener noreferrer">
                    <span class="icon">
                      🤗
                    </span>
                    <span>MathCritique-76k</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
  <!-- Your video here -->
  <!-- <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Training large language models (LLMs) to spend more time thinking and reflection
              before responding is crucial for effectively solving complex reasoning tasks in
              fields such as science, coding, and mathematics. However, the effectiveness of
              mechanisms like self-reflection and self-correction depends on the model’s capacity
              to accurately assess its own performance, which can be limited by factors such
              as initial accuracy, question difficulty, and the lack of external feedback. In this
              paper, we delve into a two-player paradigm that separates the roles of reasoning and
              critique models, where the critique model provides step-level feedback to supervise
              the reasoning (actor) model during both test-time and train-time. We first propose
              AutoMathCritique, an automated and scalable framework for collecting critique
              data, resulting in a dataset of responses paired with step-level feedback.
              Fine-tuning language models with this dataset enables them to generate natural
              language feedback for mathematical reasoning. We demonstrate that the critique
              models consistently improve the actor’s performance on difficult queries at test-
              time, especially when scaling up inference-time computation. Motivated by these
              findings, we introduce the critique-based supervision to the actor’s self-training
              process, and propose a critique-in-the-loop self-improvement method. Experiments
              show that the method improves the actor’s exploration efficiency and solution
              diversity, especially on challenging queries, leading to a stronger reasoning model.
              Lastly, we take the preliminary step to explore training self-talk reasoning models
              via critique data and showcase its potential.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <section class="section hero is-small is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <div class="content">
            <h2 class="title is-3">AutoMathCritique: An Automated and Scalable Framework to Collect
              Step-level Critique Data
            </h2>
            <center>
              <div class="content has-text-justified">
                <img src="static/images/AutoMathCritique.pdf-1-12.jpg" alt="MY ALT TEXT" />
              </div>
              <p>
                The overview of AutoMathCritique framework. It has three main steps: flawed reasoning path construction,
                critique generation, data filtering.
              </p>
            </center>
            <br />
            <div class="level-set has-text-justified">
              <p>
                <b>TL'DR
                </b>
                <br />
                <br />
                To train critique models capable of delivering step-level supervision and constructive feedback for
                reasoning, we introduce AutoMathCritique—an automated and scalable framework for collecting critique
                data.<br />
                This framework consists of three main stages: flawed reasoning path construction, critique generation,
                and data filtering. Using AutoMathCritique, we create a dataset containing 76321 samples named
                MathCritique-76k.
              </p>
            </div>
            <br />

            <b>Case</b>
            <center>
              <div class="content has-text-justified">
                <img src="static/images/case_study_1.pdf-1-10.jpg" alt="MY ALT TEXT" />
              </div>
              <p>Figure 2: example of the response, critique, and refinement process in the two-player setting.</p>
            </center>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <div class="content">
            <h2 class="title is-3">Critique Models Improves LLM Reasoning through Test-time Supervision
            </h2>


            <table>
              <tr>
                <th rowspan="2">Critique Model</th>
                <th colspan="3">
                  <center>GSM8K</center>
                </th>
                <th colspan="3">
                  <center>MATH</center>
                </th>
              </tr>
              <tr>
                <th>Acc</th>
                <th>Discrimin.</th>
                <th>Helpfulness</th>
                <th>Acc</th>
                <th>Discrimin.</th>
                <th>Helpfulness</th>
              </tr>
              <tr>
                <td>No Critic</td>
                <td>54.81</td>
                <td>-</td>
                <td>-</td>
                <td>17.22</td>
                <td>-</td>
                <td>-</td>
              </tr>
              <tr>
                <td>GPT-3.5-Turbo</td>
                <td>58.38</td>
                <td>62.9%</td>
                <td>13.3%</td>
                <td>25.56</td>
                <td>51.3%</td>
                <td>14.3%</td>
              </tr>
              <tr>
                <td>GPT-4-Turbo</td>
                <td>77.86</td>
                <td>91.6%</td>
                <td>57.5%</td>
                <td>36.00</td>
                <td>87.6%</td>
                <td>26.2%</td>
              </tr>
              <tr>
                <td>GPT-4o</td>
                <td>79.52</td>
                <td>91.5%</td>
                <td>59.7%</td>
                <td>39.98</td>
                <td>85.4%</td>
                <td>30.9%</td>
              </tr>
              <tr>
                <td>Critique Model-8b</td>
                <td>63.31</td>
                <td>79.4%</td>
                <td>31.0%</td>
                <td>24.26</td>
                <td>75.7%</td>
                <td>16.2%</td>
              </tr>
              <tr>
                <td>Critique Model-70b</td>
                <td>76.88</td>
                <td>92.3%</td>
                <td>55.3%</td>
                <td>33.94</td>
                <td>82.3%</td>
                <td>23.9%</td>
              </tr>
            </table>
            <p>
              Test-time evaluation results of critique models on GSM8K and MATH. "Acc" represents accuracy; "Discrimin."
              refers to the accuracy of determining whether a reasoning path contains errors; "Helpfulness" indicates
              the ability of critique models to provide assistance for an incorrect reasoning path. The "No Critic"
              baseline represents the standalone performance of the actor reasoning model. Our 8B critique model
              outperforms GPT-3.5-Turbo, while the 70B critique model achieves performance close to the GPT-4 series
              models.
            </p>
            <center>
              <img src="static/images/difficulty_disrtibution.pdf-1-8.jpg" alt="Mixed Video-Image Finetuning"
                class="center-image blend-img-background" />
            </center>
            <div class="level-set has-text-justified">
              <p>
                To construct the reasoning process in our two-player setting, First, we train critique models through
                super-vised fine-tuning with the collected MathCritique-76k.In this way, we can obtain a critique model
                that provides step-level supervision and constructive feedback on reasoning paths for actor models.We
                then train reasoning models in our two-player setting. We mixed reasoning data and refinement data in
                the training set to enable the model to possess reasoning abilities while also effectively refining
                based on critique feedback.
                <br />
                <br />
                Here are our findings in our test-time improvement experiments:
              </p>
            </div>
            <h3 class="title is-4">Critique models are highly effective at identifying the correctness of reasoning,
              offering constructive feedback for erroneous responses, and improving the overall accuracy of the actor.
            </h3>
            <p>
              We compare our critique models with SOTA models used as critics, and the results are presented in Table 2.
              We observe that compared to current state-of-the-art (SOTA) models, our 8B critique model significantly
              outperforms GPT-3.5, while our Llama3-Critic-70B model achieves performance comparable to GPT-4 series
              models.
            </p>
            <h3 class="title is-4">Critique models assist the actor in better handling challenging queries.
            </h3>
            <p>
              The results are illustrated in Figure . is evident that on both the training and test sets of GSM8K and
              MATH, critique models provide minimal benefit for simpler queries, as the actor model can independently
              perform well on these cases. However, for more challenging problems, critique models offer significant
              support, resulting in the overall improved performance. </p>
            <h3 class="title is-4">Scaling up inference-time computation consistently improves reasoning performance.
            </h3>
            <p>
              We investigate whether incorporating critique models can further elevate the reasoning performance ceiling
              as test-time computation scales.As shown in Figure , without critique models, Maj@K performance improves
              with increased computation but quickly plateau.In contrast, when critique models are utilized during
              test-time, performance surpasses the baseline by a significant margin under the same computation
              budget—showing a 12.4% improvement on GSM8K and a 14.8% improvement on MATH. </p>
          </div>
        </div>
      </div>
    </div>
  </section>




  <section class="section hero is-small">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <div class="content">
            <h2 class="title is-3">Critique-in-the-loop Self-Improvement for Better Reasoning Models
            </h2>


            <p>
              Motivated by the test-time findings in Test-time improve that critique models significantly aid in solving
              challenging problems, and that they substantially raise the reaosning performance ceiling when scaling up
              computation, we integrate the critique-based supervision into the actor model’s iterative exploration and
              learning process. We introduce a critique-in-the-loop self-improvement method,which scales up exploration
              computation on challenging queries and leads to the development of stronger reasoning models. </p>
            <!-- 公式todu -->
            <center>
              <img src="static/images/algorithm.png" alt="Mixed Video-Image Finetuning"
                class="center-image blend-img-background" />
            </center>
            <center>
              <img src="static/images/self_improve_iteration.pdf-1-4.jpg" alt="Mixed Video-Image Finetuning"
                class="center-image blend-img-background" />
            </center>
            <div class="level-set has-text-justified">
              <p>
                The evaluating results of critique-in-the-loop self-improvement. "SI" in the figure means
                self-improvement.
                Compared to the vanilla self-improvement approach, our method achieves significant performance
                improvements, particularly at larger N values.
                <br />
                <br />
                Here are our main findings :
              </p>
            </div>
            <h3 class="title is-4">Critique-in-the-loop self-improvement consistently improves reasoning performance.
            </h3>
            <p>
              The evaluating results of our method are shown in Figure 6. We can observe that: (1) Increasing the number
              of samples during exploration improves performance, with the performance upper bound rising accordingly,
              underscoring the benefits of enhanced exploration computation. (2) Our method consistently outperforms
              vanilla self-improvement with stable and significant performance gains, especially when the sample number
              N is larger. </p>
              <center>
                <img src="static/images/difficulty_propotion.pdf-1-7.jpg" alt="Mixed Video-Image Finetuning"
                  class="center-image blend-img-background" />
                  <p>The difference in the proportion of training data across different difficulty levels obtained from the exploration steps of the critique-in-the-loop self-improvement method compared to the data obtained from the exploration steps of the vanilla self-improvement method. On both datasets, our method increases the proportion of difficult problems in the training set while reducing the proportion of simpler problems.</p>
              </center>
              <center>
                <img src="static/images/train_time_difficult_acc.pdf-1-1.jpg" alt="Mixed Video-Image Finetuning"
                  class="center-image blend-img-background" />
                  <p>The performance differences between our method and vanilla self-improvement on test sets of varying difficulty. While our method slightly outperforms the vanilla approach on simpler problems, it achieves significantly greater improvements on harder problems.</p>
              </center>
    
            <h3 class="title is-4">Critique-in-the-loop self-improvement balances the solution distribution across difficulty levels,and enhances performance on challenging queries in the test set. 
            </h3>
            <p>
              As shown in Figure 7, we find that our approach samples a higher proportion of solutions for challenging queries during the exploration stage. This significantly balances the training data distribution for the learning stage, effectively mitigating the tail narrowing issue. In Figure 8, we also present the model’s performance on the test set across different difficulty levels, and we observe that our method performs significantly better than the vanilla approach on harder problems, further demonstrating the potential of our approach.</p>
            <h3 class="title is-4">Combining test-time supervision with training-time supervisions yields more performance gains.
            </h3>
            <center>
              <table>
                <tr>
                  <th>Training-time</th>
                  <th>Test-time</th>
                  <th colspan="3">GSM8K</th>
                  <th colspan="3">MATH</th>
                </tr>
                <tr>
                  <th>Training-time</th>
                  <th>Test-time</th>
                  <th>Acc</th>
                  <th>Pass@5</th>
                  <th>MV@5</th>
                  <th>Acc</th>
                  <th>Pass@5</th>
                  <th>MV@5</th>
                </tr>
                <tr>
                  <td>response only</td>
                  <td>response only</td>
                  <td>54.8</td>
                  <td>75.2</td>
                  <td>54.5</td>
                  <td>17.2</td>
                  <td>35.0</td>
                  <td>15.6</td>
                </tr>
                <tr>
                  <td>Supervised Fine-tuning</td>
                  <td>w/ critique model</td>
                  <td>63.3</td>
                  <td>87.6</td>
                  <td>75.4</td>
                  <td>24.3</td>
                  <td>47.4</td>
                  <td>30.7</td>
                </tr>
                <tr>
                  <td>Supervised Fine-tuning</td>
                  <td>response only</td>
                  <td>54.2</td>
                  <td>73.1</td>
                  <td>53.4</td>
                  <td>18.1</td>
                  <td>32.4</td>
                  <td>16.6</td>
                </tr>
                <tr>
                  <td>Self-Correction Fine-tuning</td>
                  <td>self-correction</td>
                  <td>60.1</td>
                  <td>81.5</td>
                  <td>67.2</td>
                  <td>24.2</td>
                  <td>41.7</td>
                  <td>26.1</td>
                </tr>
                <tr>
                  <td>Self-Correction Fine-tuning</td>
                  <td>response only</td>
                  <td>64.6</td>
                  <td>83.4</td>
                  <td>70.6</td>
                  <td>20.2</td>
                  <td>38.5</td>
                  <td>23.0</td>
                </tr>
                <tr>
                  <td>Vanilla Self-Improve</td>
                  <td>w/ critique model</td>
                  <td>70.2</td>
                  <td  style="text-decoration: underline;">90.8</td>
                  <td>78.2</td>
                  <td>27.0</td>
                  <td>48.8</td>
                  <td>31.4</td>
                </tr>
                <tr>
                  <td>Vanilla Self-Improve</td>
                  <td>response only</td>
                  <td  style="text-decoration: underline;">75.5</td>
                  <td>89.1</td>
                  <td  style="text-decoration: underline;">80.1</td>
                  <td  style="text-decoration: underline;">31.3</td>
                  <td  style="text-decoration: underline;">51.0</td>
                  <td  style="text-decoration: underline;">35.1</td>
                </tr>
                <tr>
                  <td>Critique-in-the-loop Self-Improve</td>
                  <td>w/ critique model</td>
                  <td  style="text-decoration: underline; font-weight: bold;">75.8</td>
                  <td  style="text-decoration: underline; font-weight: bold;">91.8</td>
                  <td  style="text-decoration: underline; font-weight: bold;">82.8</td>
                  <td  style="text-decoration: underline; font-weight: bold;">31.4</td>
                  <td  style="text-decoration: underline; font-weight: bold;">53.1</td>
                  <td  style="text-decoration: underline; font-weight: bold;">36.8</td>
                </tr>
              </table>
              
              <p>Evaluation results of combining different training-time and test-time methods. During training, "Self-Correction Fine-tuning" refers to training a model with both reasoning and correction capabilities. For test-time methods, "response only" represents the actor model generating a response without additional correction or critique; "w/ critique model" indicates using a critique model at test-time to provide feedback, enabling the actor to perform refinement; and "self-correction" refers to the model generating a response and then performing correction by itself. The best performance is in bold and underlined, while the second-best performance is underlined.From the results, we observe that both test-time and train-time critique supervision provide consistent improvements, and the combination of the two achieves the best performance.</p>

            </center>
            <br/>
            <p>
              
              Evaluation results shown in Table 3 reveal that: (1) Integrating critique models during test-time consistently enhances performance under identical training conditions, particularly when critique supervision is not used during training. For example, applying critique models at test-time increases the MV@5 performance of SFT on GSM8K and MATH by $10.9$ and $15.1$ points, respectively. (2) When critique models are used during training, the additional benefit of test-time critique supervision becomes marginal, suggesting successful “distillation” of critique models into the actor during training. (3) The self-correction baseline underperforms compared to utilizing separate critique models, aligning findings in prior work that models struggle to accurately evaluate and refine their outputs without external feedback. Moreover, training a single model to handle both reasoning and correction capabilities may introduce conflicts, leading to performance degradation. (4) Compared to the traditional strategy of vanilla self-improvement + response-only, which increases computation during training, the approach of supervised fine-tuning + test-time critique supervision reduces training computation while increasing test-time computation and achieves better performance, particularly on the more challenging MATH dataset. This aligns with prior work highlighting the benefits of enhancing test-time computation.
              </p>
            </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section hero is-small">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <div class="content">
            <h2 class="title is-3">  A Step Further: Training Step-level Self-Talk Reasoning Models via Critique Data

            </h2>

            <center>
              <img src="static/images/Method-selftalk.png" alt="Mixed Video-Image Finetuning"
                class="center-image blend-img-background" />
            </center>


            <h3 class="title is-4">Motivation 
            </h3>
            <p>In this work, we focus on the two-player paradigm, leveraging critique models to provide step-level supervision and feedback for actor models. Recently, OpenAI’s o1 model has pushed the boundaries of large reasoning models’ capabilities. With its self-talk output format, it can autonomously plan, reflect, critique, correct, backtrack, and more during the thinking process, marked by phrases such as "wait" and "alternatively". </p>
            <h3 class="title is-4">Method
            </h3>
            <p>
            Therefore, we investigate whether it is possible to construct self-talk data with step-level critique supervision, and propose the preliminary self-talk-via-critique method. Specifically, it has three main steps:
            <ol>
            <li>Construct an initial thinking chain that has step-level reflection.</li>
            <li>Iterative refine and critique the thinking chain.</li>
            <li>Smooth the thinking chain into self-talk data.</li>
          </ol>
            </p>
            <h3 class="title is-4">Evaluation and findings.
            </h3>

            <p>
              Based on this approach, we construct a dataset of 4k self-talk examples from the MATH training set and fine-tune the model for evaluation. As in the previous section, we used the Llama3-8b base model as the backbone for our experiments. We compared our method with the self-correction baseline and the baseline of SFT with test-time critique supervision. These two baselines fall under the one-player and two-player settings, respectively. Note that for the two baselines, we only used the MATH dataset for training, without using GSM8K data. The experimental results are shown in Table 5. We observe that, in the one-player setting, the step-level self-talk approach outperforms trajectory-level self-correction by a significant margin, demonstrating its potential. However, it still lags behind the two-player setting, indicating that this direction requires further exploration, which we leave to future work.            </p>
            <br/>
            <b>Case</b>
              <center>
              <img src="static/images/case_study_2.pdf-1-9.jpg" alt="Mixed Video-Image Finetuning"
                class="center-image blend-img-background" />
                <p>An example of the constructed step-leval self-talk data.</p>
            </center>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!--BibTex citation -->
  <section class="section is-light" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{xi2024enhancingllmreasoningcritique,
      title={Enhancing LLM Reasoning via Critique Models with Test-Time and Training-Time Supervision}, 
      author={Zhiheng Xi and Dingwen Yang and Jixuan Huang and Jiafu Tang and Guanyu Li and Yiwen Ding and Wei He and Boyang Hong and Shihan Do and Wenyu Zhan and Xiao Wang and Rui Zheng and Tao Ji and Xiaowei Shi and Yitao Zhai and Rongxiang Weng and Jingang Wang and Xunliang Cai and Tao Gui and Zuxuan Wu and Qi Zhang and Xipeng Qiu and Xuanjing Huang and Yu-Gang Jiang},
      year={2024},
      eprint={2411.16579},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.16579}, 
}</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->




  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
